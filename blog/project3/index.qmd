---
title: "Multinomial Logit Model"
author: Emma Wu
date: today
---


This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$


## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{}
#| eval: false

# set seed for reproducibility
set.seed(123)

# define attributes
brand <- c("N", "P", "H") # Netflix, Prime, Hulu
ad <- c("Yes", "No")
price <- seq(8, 32, by=4)

# generate all possible profiles
profiles <- expand.grid(
    brand = brand,
    ad = ad,
    price = price
)
m <- nrow(profiles)

# assign part-worth utilities (true parameters)
b_util <- c(N = 1.0, P = 0.5, H = 0)
a_util <- c(Yes = -0.8, No = 0.0)
p_util <- function(p) -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps <- 100
n_tasks <- 10
n_alts <- 3

# function to simulate one respondent’s data
sim_one <- function(id) {
  
    datlist <- list()
    
    # loop over choice tasks
    for (t in 1:n_tasks) {
        
        # randomly sample 3 alts (better practice would be to use a design)
        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])
        
        # compute deterministic portion of utility
        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat$e <- -log(-log(runif(n_alts)))
        dat$u <- dat$v + dat$e
        
        # identify chosen alternative
        dat$choice <- as.integer(dat$u == max(dat$u))
        
        # store task
        datlist[[t]] <- dat
    }
    
    # combine all tasks for one respondent
    do.call(rbind, datlist)
}

# simulate data for all respondents
conjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))

# remove values unobservable to the researcher
conjoint_data <- conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]

# clean up
rm(list=setdiff(ls(), "conjoint_data"))
```
::::


## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

```{python}
#| code-fold: true
#| code-summary: "Code"
#| output: false
import pandas as pd
df = pd.read_csv('conjoint_data.csv')

# Step 1: Create dummy variables for brand and ad (drop reference level)
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Step 2: Define feature matrix X
# Reference: brand_H, ad_No are omitted
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]]

# Step 3: Add intercept manually
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]  # ensure order

# Step 4: Create outcome vector y (binary: 1 if chosen, 0 otherwise)
y = df_encoded["choice"]

# Step 5: Group metadata (optional but useful for tracking)
groups = df_encoded[["resp", "task"]]

# Preview
X.head(), y.head(), groups.head()

```


## 4. Estimation via Maximum Likelihood

We estimate the model using Maximum Likelihood. First, we define the log-likelihood function based on the MNL specification, then use numerical optimization to find the MLEs. Finally, we use the inverse Hessian to obtain standard errors and construct 95% confidence intervals.

### Log-Likelihood Function
```{python}
#| output: false
import numpy as np
from scipy.special import logsumexp
import pandas as pd
df = pd.read_csv('conjoint_data.csv')

df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]]
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]  # ensure order
y = df_encoded["choice"]
groups = df_encoded[["resp", "task"]]

# Reshape for 3-alternative tasks
X_np = X.values
y_np = y.values
X_tasks = X_np.reshape((-1, 3, X_np.shape[1]))
y_tasks = y_np.reshape((-1, 3))

def neg_log_likelihood(beta):
    """Negative log-likelihood for MNL."""
    X_tasks = X_tasks.astype(np.float64)
    beta = np.asarray(beta, dtype=np.float64)
    utilities = np.einsum("tjk,k->tj", X_tasks, beta)
    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)
    return -np.sum(chosen_log_probs)
```

### Optimization and Estimation Results
```{python}
#| output: false
import numpy as np
import pandas as pd
from scipy.special import logsumexp
from scipy.optimize import minimize
from numpy.linalg import inv

df = pd.read_csv("conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]].copy()
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]
y = df_encoded["choice"]
X_np = X.values.astype(np.float64)
y_np = y.values.astype(np.float64)
X_tasks = X_np.reshape((-1, 3, X_np.shape[1]))
y_tasks = y_np.reshape((-1, 3))

def neg_log_likelihood(beta):
    utilities = np.einsum("tjk,k->tj", X_tasks, beta)
    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)
    return -np.sum(chosen_log_probs)
result = minimize(
    fun=neg_log_likelihood,
    x0=np.zeros(X_np.shape[1]),
    method="BFGS"
)

beta_hat = result.x
hessian_inv = result.hess_inv
std_err = np.sqrt(np.diag(hessian_inv))
z = 1.96
ci_lower = beta_hat - z * std_err
ci_upper = beta_hat + z * std_err

param_names = ["Intercept", "Netflix", "Prime", "Ad", "Price"]
results_df = pd.DataFrame({
    "Estimate": beta_hat,
    "Std. Error": std_err,
    "95% CI Lower": ci_lower,
    "95% CI Upper": ci_upper
}, index=param_names).round(4)
```

The table below summarizes the estimated coefficients from the MNL model, along with standard errors and 95% confidence intervals.

| Variable   | Estimate | Std. Error | 95% CI Lower | 95% CI Upper |
|------------|----------|------------|---------------|---------------|
| Intercept  | 0.0000   | 1.0000     | -1.9600       | 1.9600        |
| Netflix    | 0.9412   | 0.1181     | 0.7097        | 1.1727        |
| Prime      | 0.5016   | 0.1207     | 0.2651        | 0.7382        |
| Ad         | -0.7320  | 0.0893     | -0.9071       | -0.5569       |
| Price      | -0.0995  | 0.0063     | -0.1119       | -0.0871       |

The results indicate strong preferences for Netflix and Prime (relative to Hulu), a penalty for ads, and a negative effect of price. All coefficients are statistically significant except for the intercept.

## 5. Estimation via Bayesian Methods

### Posterior Simulation: Metropolis-Hastings

We use a Metropolis-Hastings MCMC sampler to simulate draws from the posterior distribution of our model parameters. We take 11,000 steps, discard the first 1,000 as burn-in, and retain 10,000 posterior draws for inference.

The proposal distribution is a multivariate normal with independent dimensions. The first three parameters (Netflix, Prime, Ad) use a standard deviation of 0.05, and the price coefficient uses 0.005, reflecting tighter prior belief.

Our sampler achieved an acceptance rate of **56.40%**, which is in the desirable range for good mixing.

```{python}
#| code-fold: true
#| code-summary: "Code"
#| output: false
import numpy as np
import pandas as pd
from scipy.special import logsumexp
from scipy.optimize import minimize

# Load and encode data
df = pd.read_csv("conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Create feature matrix X
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]].copy()
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]

# Create target vector y
y = df_encoded["choice"]

# Convert to NumPy arrays
X_np = X.values.astype(np.float64)
y_np = y.values.astype(np.float64)
X_tasks = X_np.reshape((-1, 3, X_np.shape[1]))
y_tasks = y_np.reshape((-1, 3))

# Log-likelihood function
def neg_log_likelihood(beta):
    utilities = np.einsum("tjk,k->tj", X_tasks, beta)
    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)
    return -np.sum(chosen_log_probs)

# Log-prior (N(0,5) for binary, N(0,1) for price)
def log_prior(beta):
    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)
    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)
    return np.sum(binary_prior) + price_prior

# Log-posterior = log-likelihood + log-prior
def log_posterior(beta):
    return -neg_log_likelihood(beta) + log_prior(beta)

# Metropolis-Hastings MCMC sampler
def metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):
    n_params = len(initial_beta)
    samples = np.zeros((steps, n_params))
    accepted = 0
    current_beta = initial_beta
    current_log_post = log_posterior(current_beta)

    if proposal_scales is None:
        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])

    for step in range(steps):
        proposal = current_beta + np.random.normal(scale=proposal_scales)
        proposal_log_post = log_posterior(proposal)
        accept_ratio = np.exp(proposal_log_post - current_log_post)
        if np.random.rand() < accept_ratio:
            current_beta = proposal
            current_log_post = proposal_log_post
            accepted += 1
        samples[step] = current_beta

    print(f"Acceptance rate: {accepted / steps:.2%}")
    return samples

# Run MCMC
initial_beta = np.zeros(X_np.shape[1])
proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])
samples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)

# Discard burn-in
posterior_samples = samples[1000:]
posterior_samples
```

The result is a NumPy array of shape (10,000, 5), where each column represents a parameter:

| Index | Parameter |
|-------|-----------|
| 0     | Intercept |
| 1     | Netflix   |
| 2     | Prime     |
| 3     | Ad        |
| 4     | Price     |

### Posterior Visualization: Price Coefficient

We used a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the Price coefficient. The plots below include:

- A trace plot to assess convergence behavior
- A posterior histogram to visualize the shape and spread of the sampled values

```{python}
#| code-fold: true
#| code-summary: "Code"
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from scipy.special import logsumexp
from scipy.optimize import minimize

# Load and encode data
df = pd.read_csv("conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Create feature matrix X
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]].copy()
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]

# Create target vector y
y = df_encoded["choice"]

# Convert to NumPy arrays
X_np = X.values.astype(np.float64)
y_np = y.values.astype(np.float64)
X_tasks = X_np.reshape((-1, 3, X_np.shape[1]))
y_tasks = y_np.reshape((-1, 3))

# Log-likelihood function
def neg_log_likelihood(beta):
    utilities = np.einsum("tjk,k->tj", X_tasks, beta)
    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)
    return -np.sum(chosen_log_probs)

# Log-prior (N(0,5) for binary, N(0,1) for price)
def log_prior(beta):
    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)
    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)
    return np.sum(binary_prior) + price_prior

# Log-posterior = log-likelihood + log-prior
def log_posterior(beta):
    return -neg_log_likelihood(beta) + log_prior(beta)

# Metropolis-Hastings MCMC sampler
def metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):
    n_params = len(initial_beta)
    samples = np.zeros((steps, n_params))
    accepted = 0
    current_beta = initial_beta
    current_log_post = log_posterior(current_beta)

    if proposal_scales is None:
        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])

    for step in range(steps):
        proposal = current_beta + np.random.normal(scale=proposal_scales)
        proposal_log_post = log_posterior(proposal)
        accept_ratio = np.exp(proposal_log_post - current_log_post)
        if np.random.rand() < accept_ratio:
            current_beta = proposal
            current_log_post = proposal_log_post
            accepted += 1
        samples[step] = current_beta

    print(f"Acceptance rate: {accepted / steps:.2%}")
    return samples

# Run MCMC
initial_beta = np.zeros(X_np.shape[1])
proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])
samples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)

# Discard burn-in
posterior_samples = samples[1000:]
posterior_samples

# Set parameter names
param_names = ["Intercept", "Netflix", "Prime", "Ad", "Price"]

# Choose one parameter to plot (e.g., Price = index 4)
param_index = 4
param_label = param_names[param_index]
samples_param = posterior_samples[:, param_index]

# Create figure with trace and histogram
fig, axes = plt.subplots(2, 1, figsize=(10, 6), constrained_layout=True)

# Trace plot
axes[0].plot(samples_param, alpha=0.8, linewidth=0.8)
axes[0].set_title(f"Trace Plot for {param_label} Coefficient")
axes[0].set_ylabel("Value")
axes[0].set_xlabel("Iteration")

# Histogram
sns.histplot(samples_param, bins=50, kde=True, ax=axes[1], color="skyblue")
axes[1].set_title(f"Posterior Distribution of {param_label} Coefficient")
axes[1].set_xlabel("Coefficient Value")
axes[1].set_ylabel("Frequency")

plt.tight_layout()
plt.show()
```

- The trace plot shows stable, well-mixed values with no drift — indicating convergence and good mixing.
- The posterior distribution is approximately normal and centered near −0.10.
- This supports our interpretation that increasing price reduces utility, aligning well with economic theory and the MLE result.

| Feature             | Observation                             |
|---------------------|------------------------------------------|
| Trace behavior      | Well-mixed, stationary — good convergence |
| Posterior shape     | Bell-shaped, symmetric                   |
| Posterior center    | Around −0.10                            |
| Interpretation      | Price reduces utility in choice behavior |

### Posterior Summary: Bayesian Estimation via MCMC

After running 11,000 Metropolis-Hastings iterations and discarding the first 1,000 as burn-in, we retained 10,000 samples from the posterior distribution for each parameter.

```{python}
#| code-fold: true
#| code-summary: "Code"
#| output: false
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from scipy.special import logsumexp
from scipy.optimize import minimize

# Load and encode data
df = pd.read_csv("conjoint_data.csv")
df_encoded = pd.get_dummies(df, columns=["brand", "ad"], drop_first=True)

# Create feature matrix X
X = df_encoded[["brand_N", "brand_P", "ad_Yes", "price"]].copy()
X["intercept"] = 1
X = X[["intercept", "brand_N", "brand_P", "ad_Yes", "price"]]

# Create target vector y
y = df_encoded["choice"]

# Convert to NumPy arrays
X_np = X.values.astype(np.float64)
y_np = y.values.astype(np.float64)
X_tasks = X_np.reshape((-1, 3, X_np.shape[1]))
y_tasks = y_np.reshape((-1, 3))

# Log-likelihood function
def neg_log_likelihood(beta):
    utilities = np.einsum("tjk,k->tj", X_tasks, beta)
    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)
    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)
    return -np.sum(chosen_log_probs)

# Log-prior (N(0,5) for binary, N(0,1) for price)
def log_prior(beta):
    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)
    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)
    return np.sum(binary_prior) + price_prior

# Log-posterior = log-likelihood + log-prior
def log_posterior(beta):
    return -neg_log_likelihood(beta) + log_prior(beta)

# Metropolis-Hastings MCMC sampler
def metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):
    n_params = len(initial_beta)
    samples = np.zeros((steps, n_params))
    accepted = 0
    current_beta = initial_beta
    current_log_post = log_posterior(current_beta)

    if proposal_scales is None:
        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])

    for step in range(steps):
        proposal = current_beta + np.random.normal(scale=proposal_scales)
        proposal_log_post = log_posterior(proposal)
        accept_ratio = np.exp(proposal_log_post - current_log_post)
        if np.random.rand() < accept_ratio:
            current_beta = proposal
            current_log_post = proposal_log_post
            accepted += 1
        samples[step] = current_beta

    print(f"Acceptance rate: {accepted / steps:.2%}")
    return samples

# Run MCMC
initial_beta = np.zeros(X_np.shape[1])
proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])
samples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)

# Discard burn-in
posterior_samples = samples[1000:]
# Compute posterior statistics
posterior_mean = np.mean(posterior_samples, axis=0)
posterior_std = np.std(posterior_samples, axis=0)
posterior_ci_lower = np.percentile(posterior_samples, 2.5, axis=0)
posterior_ci_upper = np.percentile(posterior_samples, 97.5, axis=0)

# Organize into a DataFrame
param_names = ["Intercept", "Netflix", "Prime", "Ad", "Price"]
posterior_summary = pd.DataFrame({
    "Posterior Mean": posterior_mean,
    "Posterior Std. Dev": posterior_std,
    "95% Credible Interval Lower": posterior_ci_lower,
    "95% Credible Interval Upper": posterior_ci_upper
}, index=param_names).round(4)

posterior_summary
```

The table below reports the posterior mean, standard deviation, and 95% credible interval for each of the 5 model parameters.

| Parameter | Posterior Mean | Posterior Std. Dev | 95% CI Lower | 95% CI Upper |
|-----------|----------------|---------------------|--------------|--------------|
| Intercept | 0.7741         | 0.8863              | −0.7093      | 2.5075       |
| Netflix   | 0.9471         | 0.1139              | 0.7332       | 1.1767       |
| Prime     | 0.5058         | 0.1162              | 0.2877       | 0.7364       |
| Ad        | −0.7380        | 0.0860              | −0.9088      | −0.5750      |

- All posterior means are very close to your MLE estimates.
- Credible intervals are tight and consistent with maximum likelihood confidence intervals.
- The Netflix and Prime coefficients remain positive and significant, while Ad and Price are negative as expected.
- This reinforces that your MCMC sampler worked well, and confirms your results from the MLE approach.


## 6. Discussion

### Parameter Interpretation (as if data were not simulated)

Let’s suppose we were working with real-world data instead of simulated conjoint responses. Based on the parameter estimates, we can still derive meaningful insights:

- **$\beta_\text{Netflix} > \beta_\text{Prime}$**: This implies that, all else equal, consumers prefer Netflix over Amazon Prime. The higher coefficient for Netflix indicates that it contributes more to overall utility than Prime when making a choice.
- **$\beta_\text{Ad} < 0$**: This negative value suggests that respondents dislike advertisements — products with ads are less likely to be chosen than ad-free options.
- **$\beta_\text{Price} < 0$**: This makes intuitive sense: higher price reduces the likelihood of choice, all else being equal. A negative price coefficient is expected in virtually any consumer utility model.

If we hadn’t known this data was simulated, we would still reasonably conclude the following:

- Consumers exhibit clear brand preferences, with Netflix most preferred.
- Ad-free plans are consistently more attractive.
- Price is a deterrent, confirming basic economic intuition.

### Extension: Multi-Level (Hierarchical) Models

In real-world conjoint analysis, it is unrealistic to assume all consumers share the same preferences. While the standard Multinomial Logit model estimates a single set of coefficients $\beta$ for the entire population, a **multi-level (hierarchical)** model allows each individual to have their own $\beta_i$, drawn from a population distribution:

$$
\beta_i \sim \mathcal{N}(\mu, \Sigma)
$$

This structure enables us to capture heterogeneity across respondents. To simulate hierarchical data, we would draw a different $\beta_i$ for each respondent and use these individual-level coefficients to generate their choices. For estimation, we could use **hierarchical Bayesian methods** (such as Gibbs sampling or Hamiltonian Monte Carlo) or frequentist mixed logit approaches. These models provide more realistic predictions, enable personalization, and are widely used in industry to analyze large-scale conjoint data.












