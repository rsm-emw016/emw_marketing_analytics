[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nEmma Wu\n\n\nMay 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nEmma Wu\n\n\nMay 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nEmma Wu\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Karlan and List conducted a large-scale natural field experiment with over 50,000 previous donors to a liberal nonprofit in the U.S. Each donor was randomly assigned to receive one of several fundraising letters. The control group received a standard letter, while treatment groups received letters offering a matching grant, where a “concerned member” would match donations at 1:1, 2:1, or 3:1. These letters also varied in the maximum match amount ($25,000, $50,000, $100,000, or not stated) and the suggested donation amount, which was based on each donor’s past contributions.\nThis setup allowed the researchers to test whether lowering the “price” of giving through matching offers increased donations. They also studied how the effects varied by political affiliation (red vs. blue states), donor history, and demographic characteristics. The findings have practical value for fundraisers and offer new insights into altruism and public goods theory.\n\n\n\n\n\n\n\nFactor\nLevels\n\n\n\n\nMatch ratio\n1:1, 2:1, 3:1\n\n\nMaximum pledge\n$25k, $50k, $100k, or unstated\n\n\nExample ask amount\nDonor’s highest previous gift, 1.25× that gift, 1.50× that gift\n\n\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Karlan and List conducted a large-scale natural field experiment with over 50,000 previous donors to a liberal nonprofit in the U.S. Each donor was randomly assigned to receive one of several fundraising letters. The control group received a standard letter, while treatment groups received letters offering a matching grant, where a “concerned member” would match donations at 1:1, 2:1, or 3:1. These letters also varied in the maximum match amount ($25,000, $50,000, $100,000, or not stated) and the suggested donation amount, which was based on each donor’s past contributions.\nThis setup allowed the researchers to test whether lowering the “price” of giving through matching offers increased donations. They also studied how the effects varied by political affiliation (red vs. blue states), donor history, and demographic characteristics. The findings have practical value for fundraisers and offer new insights into altruism and public goods theory.\n\n\n\n\n\n\n\nFactor\nLevels\n\n\n\n\nMatch ratio\n1:1, 2:1, 3:1\n\n\nMaximum pledge\n$25k, $50k, $100k, or unstated\n\n\nExample ask amount\nDonor’s highest previous gift, 1.25× that gift, 1.50× that gift\n\n\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\nvariables_to_test = ['mrm2', 'years', 'freq', 'female']\n\n\nThis dataset contains information from a large-scale natural field experiment conducted by Karlan and List (2007) to study charitable giving behavior. It includes 50,083 observations and 51 variables related to donation behavior, treatment assignments, demographics, and geographic context.\nEach row represents an individual who received a fundraising letter. Key variables include:\n\nTreatment assignment (treatment, control, ratio, size, ask)\nDonation outcome (gave, amount, amountchange)\nDonor characteristics (e.g., female, freq, years, mrm2)\nCensus and geographic info (pwhite, median_hhincome, redcty, etc.)\n\nThere are some missing values in the demographic and geographic variables (e.g., pwhite, median_hhincome), but core variables like gave and treatment are complete.\nThis dataset supports analysis of how different fundraising strategies—such as offering matching donations or suggesting different donation amounts—affect donor response rates and amounts given.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI tested four pre-treatment variables—mrm2 (months since last donation), years (years since initial donation), freq (number of prior donations), and female (binary gender indicator)—using two methods:\n\nTwo-sample t-tests\nSimple linear regressions\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\ndf = pd.read_stata('karlan_list_2007.dta')\nvariables_to_test = ['mrm2', 'years', 'freq', 'female']\n\nresults = []\n\nfor var in variables_to_test:\n    treat = df[df['treatment'] == 1][var].dropna()\n    control = df[df['treatment'] == 0][var].dropna()\n\n    diff = treat.mean() - control.mean()\n    se = np.sqrt(treat.var(ddof=1)/len(treat) + control.var(ddof=1)/len(control))\n    t_manual = diff / se\n    df_t = len(treat) + len(control) - 2\n    p_manual = 2 * (1 - stats.t.cdf(abs(t_manual), df_t))\n\n    reg_df = df[[var, 'treatment']].dropna()\n    X = sm.add_constant(reg_df['treatment'])\n    y = reg_df[var]\n    model = sm.OLS(y, X).fit()\n    \n    results.append({\n        \"Variable\": var,\n        \"Mean Difference\": round(diff, 4),\n        \"T-stat (manual)\": round(t_manual, 4),\n        \"P-value (manual)\": round(p_manual, 4),\n        \"Coef (regression)\": round(model.params['treatment'], 4),\n        \"T-stat (regression)\": round(model.tvalues['treatment'], 4),\n        \"P-value (regression)\": round(model.pvalues['treatment'], 4)\n    })\n\nresults_df = pd.DataFrame(results)\n\nprint(\"Baseline Balance Table (Treatment vs Control):\")\ndisplay(results_df)\n\n\nBaseline Balance Table (Treatment vs Control):\n\n\n\n\n\n\n\n\n\nVariable\nMean Difference\nT-stat (manual)\nP-value (manual)\nCoef (regression)\nT-stat (regression)\nP-value (regression)\n\n\n\n\n0\nmrm2\n0.0137\n0.1195\n0.9049\n0.0137\n0.1195\n0.9049\n\n\n1\nyears\n-0.0575\n-1.0909\n0.2753\n-0.0575\n-1.1030\n0.2700\n\n\n2\nfreq\n-0.0120\n-0.1108\n0.9117\n-0.0120\n-0.1109\n0.9117\n\n\n3\nfemale\n-0.0075\n-1.7535\n0.0795\n-0.0075\n-1.7584\n0.0787\n\n\n\n\n\n\n\nBoth approaches gave consistent results: no variable showed a statistically significant difference between groups at the 95% confidence level. These findings are consistent with Table 1 of the paper, where the means and standard deviations are visually similar across treatment and control conditions.\nThis reassures us that any observed differences in outcomes later on are unlikely to be driven by pre-existing differences, reinforcing the credibility of our causal claims."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nrates = df.groupby(\"treatment\")[\"gave\"].mean()\n\nlabels = [\"Control\", \"Treatment\"]\nvalues = [rates[0], rates[1]]\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(labels, values, color=[\"lightgreen\", \"skyblue\"], edgecolor=\"black\")\nplt.ylim(0, max(values) + 0.005)  \nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rates by Group\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2,\n             height - 0.001,  \n             f\"{height:.3f}\",\n             ha='center', va='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on the barplot above, we can compare donation behavior between individuals who received different versions of a fundraising letter. Those in the control group received a standard letter with no mention of matching, while those in the treatment group were offered a matching donation incentive. Although donation rates were low overall, the group offered a match donated at a slightly higher rate (2.2%) compared to the group without a match (1.8%). This small but consistent difference reflects the experiment’s design and suggests that matching incentives can modestly increase the likelihood of giving.\n\n\nCode\nimport pandas as pd\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ntreat = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n\nt_stat, p_val = stats.ttest_ind(treat, control, equal_var=False)\n\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\nmodel = sm.OLS(y, X).fit()\n\nprint(f\"T-stat: {t_stat:.4f}, P-value: {p_val:.4f}\")\nprint(model.summary())\n\n\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nX = sm.add_constant(df[\"treatment\"])  # Add intercept\ny = df[\"gave\"]\n\nprobit_model = sm.Probit(y, X).fit()\n\nprint(probit_model.summary())\n\n\n\n\n\nMethod\nDiff (%-points)\nt / t-stat\np-value\n\n\n\n\nt-test\n0.418\n3.209\n0.001\n\n\nOLS\n0.418\n3.101\n0.002\n\n\n\nThis table compares the results of a t-test and an OLS regression, both examining whether offering a donation match increases the likelihood of giving. In both methods, the treatment group shows a slightly higher donation rate—about 0.42 percentage points more than the control group. Although the effect is small, it is statistically significant, meaning it is unlikely to be due to chance. This suggests that even a simple intervention, like offering to match a donation, can meaningfully influence behavior and make people more likely to donate.\nThen, I ran a probit regression where the outcome variable is whether a person donated (gave), and the explanatory variable is whether they were assigned to receive a matching offer (treatment). The results below are based on the latent (probit) scale.\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Define the outcome and predictor\nX = sm.add_constant(df[\"treatment\"])  # add intercept\ny = df[\"gave\"]\n\n# Run Probit regression\nprobit_model = sm.Probit(y, X).fit()\n\n# Show summary\nprint(probit_model.summary())\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoef.\nStd. Err.\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n[-2.146, -2.055]\n\n\nTreatment\n0.0868\n0.028\n3.113\n0.002\n[0.032, 0.141]\n\n\n\nThe positive and statistically significant coefficient on treatment indicates that being assigned to receive a matching offer increases the likelihood of donation. While this estimate is on the latent (probit) scale, the significance (p = 0.002) confirms a clear effect.\nTo translate this to a change in predicted probability, we can compute the marginal effect (see below).\n\n\nCode\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import Probit\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n# Fit probit model\nprobit_mod = Probit(df[\"gave\"], sm.add_constant(df[\"treatment\"])).fit()\n\n# Average marginal effect\name = probit_mod.get_margeff(at=\"overall\").summary_frame()\name.round(3)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\ndy/dx\nStd. Err.\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\ntreatment\n0.004\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\nOffering to match a donor’s gift really does make a difference. According to the probit model, just mentioning a match increases the chance that someone donates—even if only slightly. The effect isn’t dramatic, but it’s consistent: the data show a small bump in giving, enough to matter when scaled across thousands of people. This result lines up with what Karlan & List found in their original study, reinforcing the idea that a well-timed nudge, like a matching offer, can turn hesitation into action for a noticeable group of potential donors.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. To evaluate how the size of the match ratio influences donation behavior, I compared response rates across match conditions using both t-tests and logistic regression.\n\nT-Test Results\nI first conducted pairwise t-tests comparing the proportion of individuals who donated (gave) under different match ratios:\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_1samp\n\n# Load and filter to treatment group\ndf = pd.read_stata('karlan_list_2007.dta')\ntreatment_only = df[df['treatment'] == 1].copy()\n\n# Create table to summarize each match ratio group\nsummary = []\n\nfor ratio_val in [1, 2, 3]:\n    group = treatment_only[treatment_only['ratio'] == ratio_val]['gave'].dropna()\n    mean_gave = group.mean()\n    std_err = group.std(ddof=1) / len(group)**0.5\n    t_stat, p_val = ttest_1samp(group, 0)  # test against 0 for formality\n\n    summary.append({\n        \"Match Ratio\": f\"{ratio_val}:1\",\n        \"Mean Gave\": round(mean_gave, 4),\n        \"N\": len(group),\n        \"Std. Error\": round(std_err, 4),\n        \"t-stat\": round(t_stat, 4),\n        \"p-value\": round(p_val, 4)\n    })\n\nresults_df = pd.DataFrame(summary)\nresults_df\n\n\n\n\n\n\n\n\n\nMatch Ratio\nMean Gave\nN\nStd. Error\nt-stat\np-value\n\n\n\n\n0\n1:1\n0.0207\n11133\n0.0014\n15.3582\n0.0\n\n\n1\n2:1\n0.0226\n11134\n0.0014\n16.0565\n0.0\n\n\n2\n3:1\n0.0227\n11129\n0.0014\n16.0892\n0.0\n\n\n\n\n\n\n\nAll three match ratio groups — 1:1, 2:1, and 3:1 — have statistically significant donation rates, with p-values effectively equal to zero. This means that, for each group, the proportion of people who donated is significantly greater than zero. However, while the donation rates increase slightly from 1:1 (2.07%) to 3:1 (2.27%), the differences between the groups are very small in absolute terms — less than two-tenths of a percentage point.\nThis result suggests that offering any matching donation significantly boosts response rates compared to no offer, but increasing the match ratio beyond 1:1 offers little to no additional benefit. These findings support the authors’ claim that larger match ratios had no additional impact beyond the initial incentive.\n\n\n\nLogistic Regression\nNext, I ran a logistic regression to model the probability of donating as a function of match ratio:\n\n\nCode\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Load data and filter to treatment group only\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nmt = df[df[\"treatment\"] == 1].copy()\n\n# Create clean dummy for 1:1 ratio\nmt[\"ratio1\"] = (mt[\"ratio\"] == 1).astype(int)\n# ratio2 and ratio3 are assumed to already exist as binary flags\n\n# Run OLS without an intercept → each coef = group mean\nreg = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=mt).fit()\n\n# Print result\nreg.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nratio1\n0.0207\n0.0014\n14.9122\n0.0\n0.0180\n0.0235\n\n\nratio2\n0.0226\n0.0014\n16.2671\n0.0\n0.0199\n0.0254\n\n\nratio3\n0.0227\n0.0014\n16.3354\n0.0\n0.0200\n0.0255\n\n\n\n\n\n\n\nThe table summarizes how donation rates vary across different match ratio offers (1:1, 2:1, 3:1). Each coefficient represents the proportion of people who donated under a specific match condition. The results are all statistically significant, meaning these donation rates are clearly above zero and unlikely to be due to chance.\n\n1:1 match: 2.07% of people donated\n2:1 match: 2.26% donated\n3:1 match: 2.27% donated\n\nAlthough donation rates are slightly higher with larger match ratios, the differences between the groups are very small—less than 0.2 percentage points. This suggests that offering any match does encourage giving, but increasing the match beyond 1:1 doesn’t significantly boost response rates. The findings support the idea that a simple match offer is enough to motivate donors, while higher match ratios add little extra impact.\n\n\n\nInterpretation of Coefficients\nTo understand how match size affects giving, I compared donation rates between the 1:1, 2:1, and 3:1 match groups using two methods:\n\nDirect from the data: I calculated the mean of the gave variable for each match group (within the treatment group).\nFrom the regression model: Since each coefficient in the no-intercept OLS regression represents the donation rate for that group, I subtracted the coefficients to compute differences.\n\n\n\nShow code for response-rate differences\nimport pandas as pd\nfrom scipy.stats import ttest_1samp\n\n# Load and filter treatment group\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ntreated = df[df[\"treatment\"] == 1].copy()\n\n# Calculate mean donation rates for each match ratio\nmeans = treated.groupby(\"ratio\")[\"gave\"].mean()\n\n# Extract donation rates\nrate_1_1 = means.loc[1]\nrate_2_1 = means.loc[2]\nrate_3_1 = means.loc[3]\n\n# Compute differences (in percentage points)\ndiff_21_11 = round((rate_2_1 - rate_1_1) * 100, 2)\ndiff_31_21 = round((rate_3_1 - rate_2_1) * 100, 2)\n\ndiff_table = pd.DataFrame({\n    \"Comparison\": [\"2:1 – 1:1\", \"3:1 – 2:1\"],\n    \"Difference (pp) direct\": [diff_21_11, diff_31_21],\n    \"Difference (pp) from reg\": [diff_21_11, diff_31_21]\n})\n\ndiff_table\n\n\n/var/folders/yf/g_800hts37z9ftxfvzrc1fc80000gn/T/ipykernel_65893/1559164577.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  means = treated.groupby(\"ratio\")[\"gave\"].mean()\n\n\n\n\n\n\n\n\n\nComparison\nDifference (pp) direct\nDifference (pp) from reg\n\n\n\n\n0\n2:1 – 1:1\n0.19\n0.19\n\n\n1\n3:1 – 2:1\n0.01\n0.01\n\n\n\n\n\n\n\n\n\n\nConclusion\nAlthough donation rates are statistically significant in all groups, the differences between them are small. This suggests that simply offering a match boosts giving, but increasing the match ratio beyond 1:1 doesn’t significantly improve donation response. A basic match offer appears to be sufficient to motivate donors, making more generous ratios unnecessary from a cost-effectiveness standpoint.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nUnconditional Analysis (All Participants)\nI first analyzed whether individuals in the treatment group donated more on average than those in the control group, regardless of whether they donated.\n\n\n\nMetric\nValue\n\n\n\n\nMean (Control Group)\n$0.813\n\n\nMean (Treatment Group)\n$0.967\n\n\nMean Difference (Treatment - Control)\n+$0.154\n\n\nP-value (T-Test)\n0.055\n\n\nTreatment Coefficient (Regression)\n0.1536 (p = 0.063)\n\n\n\nAlthough the treatment group gave slightly more, the difference is only marginally insignificant at the 5% level. This suggests the treatment may have increased average giving slightly, but the evidence is not conclusive. Since treatment was randomly assigned, this result can be interpreted as the causal effect of the treatment on the average donation amount.\n\n\nConditional Analysis (Donors Only)\nNext, I restricted the sample to only those who donated and repeated the analysis to evaluate the treatment’s effect on donation size among givers.\n\n\n\nMetric\nValue\n\n\n\n\nMean (Control Group)\n$45.54\n\n\nMean (Treatment Group)\n$43.87\n\n\nMean Difference (Treatment - Control)\n-$1.67\n\n\nP-value (T-Test)\n0.599\n\n\nTreatment Coefficient (Regression)\n−1.668 (p = 0.561)\n\n\n\nThe treatment group gave slightly less, but the difference is not statistically significant. There is no causal interpretation. Since we’re conditioning on a post-treatment behavior (having donated), this breaks randomization and introduces selection bias. Therefore, this result is descriptive only, not causal.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\ndonated_df = df[df['gave'] == 1]\n\ntreat = donated_df[donated_df['treatment'] == 1]['amount'].dropna()\ncontrol = donated_df[donated_df['treatment'] == 0]['amount'].dropna()\n\nmean_treat = treat.mean()\nmean_control = control.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Treatment plot\naxes[0].hist(treat, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='--', linewidth=2, label=f'Mean = ${mean_treat:.2f}')\naxes[0].set_title('Treatment Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control plot\naxes[1].hist(control, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean = ${mean_control:.2f}')\naxes[1].set_title('Control Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers (LLN), I simulated donation behavior using binary outcomes based on the actual probabilities observed in the experiment:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\n\ncontrol_sample = np.random.choice(control_draws, size=10000, replace=False)\n\ndifferences = treatment_draws - control_sample\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, color=\"blue\", linewidth=1)\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference 0.004')\nplt.title(\"Cumulative Average of Differences: Treatment vs. Control\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nThis plot clearly demonstrates the Law of Large Numbers in action, showing that as the number of samples increases, the cumulative average of the differences steadily converges toward the true population mean. Despite early fluctuations due to sampling noise, the average stabilizes with more observations, illustrating how even noisy individual data can produce accurate and reliable estimates of treatment effects when the sample size is sufficiently large.\n\n\nCentral Limit Theorem\nTo visualize how the sampling distribution of the average difference behaves, I simulated four experiments using increasing sample sizes: 50, 200, 500, and 1000. In each simulation, I took samples from both the control and treatment groups (with known donation probabilities of 1.8% and 2.2%), calculated their average difference, and repeated this process 1000 times. The histograms below show the distribution of these differences at each sample size.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ncontrol_p = 0.018\ntreatment_p = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\nsimulated_distributions = {}\n\nfor n in sample_sizes:\n    avg_diffs = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, control_p, n)\n        treatment = np.random.binomial(1, treatment_p, n)\n        avg_diffs.append(np.mean(treatment) - np.mean(control))\n    simulated_distributions[n] = avg_diffs\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = simulated_distributions[n]\n    axes[i].hist(diffs, bins=50, color=\"lightblue\", edgecolor=\"black\", density=True)\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n    axes[i].axvline(np.mean(diffs), color='green', linestyle='-', linewidth=2, label='Mean')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Avg Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, when the sample size is small (n = 50), the distribution is wide and noisy, and the value of zero is often near the center—indicating we might fail to detect any effect. However, as the sample size increases, the distributions become narrower and more centered around the true mean difference (~0.004), pushing zero toward the tails. This demonstrates the Central Limit Theorem: with larger sample sizes, the distribution of the sample mean becomes more normal and concentrated, making it easier to detect small but real differences in behavior."
  },
  {
    "objectID": "blog/project1/index.html#conclusion-1",
    "href": "blog/project1/index.html#conclusion-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project explored how different fundraising strategies—especially the use of matching donations—influence charitable giving. Using data from a large-scale natural field experiment, I found that simply offering a match increases both the likelihood of donating and the average donation amount. However, increasing the match ratio beyond 1:1 (to 2:1 or 3:1) provided little to no additional impact. This pattern was confirmed across multiple methods, including t-tests, linear regressions, and probit models.\nThe findings suggest that the presence of a match matters more than the size of the match. In other words, offering any match can be a powerful behavioral nudge, but bigger incentives don’t necessarily lead to bigger results. For fundraisers, this means that even simple interventions can have a meaningful effect—and they don’t always need to be expensive to be effective."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emma Wu",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('blueprinty.csv')\ndf.head()\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('blueprinty.csv')\nmeans = df.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmeans.columns = [\"Customer Status\", \"Mean Patents\"]\n\nplt.figure(figsize=(10, 5))\n\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=30,\n             element=\"step\", stat=\"density\", common_norm=False,\n             palette={0: \"skyblue\", 1: \"orange\"}, legend=True)\n\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status\nMean Patents\n\n\n\n\nNon-Customer\n3.473013\n\n\nCustomer\n4.133056\n\n\n\nFrom the chart and table, we can see that Blueprinty customers tend to have more patents than non-customers. On average, customers have 4.13 patents, while non-customers have 3.47. The histogram shows that customers are more likely to appear in the higher end of the distribution.\nHowever, it’s important to remember that customers are not randomly selected. That means we cannot say for sure that being a customer causes someone to have more patents. It’s possible that customers are already different before joining—perhaps they are more experienced, innovative, or come from regions or industries with higher patent activity.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\") \n\nmean_age = df.groupby(\"iscustomer\")[\"age\"].mean().reset_index()\nmean_age.columns = [\"Customer Status\", \"Mean Age\"]\n\nregion_counts = pd.crosstab(df[\"region\"], df[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\nregion_props = region_counts.div(region_counts.sum(axis=0), axis=1).round(3) \n\nplt.figure(figsize=(10, 4))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", bins=20, palette=[\"orange\", \"skyblue\"], element=\"step\")\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\n\n\n\n\nNon-Customer (0)\n26.10\n\n\nCustomer (1)\n26.90\n\n\n\nCustomers tend to be slightly older than non-customers. The age distribution plot shows that customers are more concentrated in the 20–35 age range, while non-customers are more evenly spread out. Although the difference is modest, it suggests that age may play a role in who becomes a customer, and should be considered when comparing outcomes like patent ownership.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\") \n\nregion_counts = pd.crosstab(df[\"region\"], df[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\nregion_props = region_counts.div(region_counts.sum(axis=0), axis=1).round(3) \n\nregion_props.plot(kind=\"bar\", figsize=(10, 5), color=[\"#fdbf6f\", \"#a6cee3\"])  \nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Proportion within Group\")\nplt.legend(title=\"Customer Status\")\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nNon-Customer\nCustomer\n\n\n\n\nMidwest\n0.184\n0.077\n\n\nNortheast\n0.268\n0.682\n\n\nNorthwest\n0.155\n0.060\n\n\nSouth\n0.153\n0.073\n\n\nSouthwest\n0.240\n0.108\n\n\n\nThere are also notable differences in regional distribution. Customers are overwhelmingly concentrated in the Northeast (68%), whereas non-customers are more evenly distributed across regions, especially in the Midwest and Southwest. This uneven geographic pattern indicates that customer status is not random and may be influenced by location-based factors. As with age, region should be taken into account when analyzing differences between customers and non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSuppose we observe independent draws \\(Y_1, Y_2, \\dots, Y_n \\sim \\text{Poisson}(\\lambda)\\),\nwhere the probability mass function of the Poisson distribution is:\n\\(f(Y_i \\mid \\lambda) = \\dfrac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThen, the likelihood function for the full sample is:\n\\(\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} f(Y_i \\mid \\lambda)\n= \\prod_{i=1}^{n} \\dfrac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\dfrac{1}{Y_i!}\\)\nSince it’s easier to work with, we usually take the logarithm of the likelihood function.\nThis gives us the log-likelihood:\n\\(\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n= -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\\)\nTo better understand the relationship between λ (lambda) and the observed data, we now walk through a full maximum likelihood estimation process for the Poisson model.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values  \n\n# ---- Step 1: Define log-likelihood function ----\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n\n# ---- Step 2: Plot lambda vs log-likelihood ----\nlambdas = np.linspace(0.1, 10, 200)\nloglik_values = [poisson_loglikelihood(lam, Y) for lam in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, loglik_values, label=\"Log-Likelihood\")\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label='MLE (mean of Y)')\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.show()\n\n# ---- Step 3: Analytical MLE solution ----\nlambda_mle_analytical = np.mean(Y)\n\n# ---- Step 4: Numerical MLE using optimization ----\nneg_loglik = lambda lam: -poisson_loglikelihood(lam, Y)\nopt_result = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle_numerical = opt_result.x\n\n# Collect results\nresults = {\n    \"Analytical MLE (mean of Y)\": round(lambda_mle_analytical, 4),\n    \"Numerical MLE (optimize)\": round(lambda_mle_numerical, 4)\n}\n\n\n\n\n\n\n\n\n\nThe plot above shows how the Poisson log-likelihood changes as we vary \\(\\lambda\\), the expected number of patents per firm. The curve reaches its peak at \\(\\lambda = 3.6847\\), which is the value that makes the observed data most likely — this is our Maximum Likelihood Estimate (MLE).\nThe table below confirms that both the analytical solution (sample mean) and the numerical optimization produce exactly the same MLE value. This gives us confidence that our model is correctly specified and our estimation is reliable.\n\n\n\nMethod\nMLE Value\n\n\n\n\nAnalytical MLE (mean of Y)\n3.6847\n\n\nNumerical MLE (optimize)\n3.6847\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\n\n# ---- Step 0: Load and prepare the data ----\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age_sq\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct the design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\ny = df[\"patents\"].values\nX_matrix = X.values\n\n# ---- Step 1: Define log-likelihood function with clipping to prevent overflow ----\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    xb = np.dot(X, beta)\n    xb = np.clip(xb, -50, 50)  # prevent overflow in exp\n    lam = np.exp(xb)\n    return np.sum(Y * xb - lam - gammaln(Y + 1))\n\n# ---- Step 2: Define negative log-likelihood for minimization ----\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\n# ---- Step 3: Numerical optimization to find MLE ----\nbeta_init = np.zeros(X.shape[1])\nresult = minimize(neg_loglikelihood, beta_init, args=(y, X_matrix), method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nse_hat = np.sqrt(np.diag(hessian_inv))\n\n# ---- Step 4: Present coefficient table ----\nresults_table = pd.DataFrame({\n    \"Coefficient\": beta_hat.round(4),\n    \"Std. Error\": se_hat.round(4)\n}, index=X.columns)\n\nresults_table\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.5100\n0.0591\n\n\nage\n0.1487\n0.0080\n\n\nage_sq\n-0.0030\n0.0002\n\n\niscustomer\n0.2076\n0.0289\n\n\nNortheast\n0.0292\n0.0540\n\n\nNorthwest\n-0.0176\n0.0668\n\n\nSouth\n0.0566\n0.0660\n\n\nSouthwest\n0.0506\n0.0597\n\n\n\n\n\nFrom our estimated Poisson regression model:\n\nThe coefficient on age is 0.1487, indicating that older firms tend to receive more patents.\nThe negative coefficient on age_sq (−0.0030) suggests diminishing returns to age — patenting increases with age but at a decreasing rate.\nThe coefficient on iscustomer is 0.2076, meaning that being a Blueprinty customer increases the log of expected patent counts by this amount.\n\nTo make this more interpretable, we exponentiate the iscustomer coefficient:\n\n\\(\\exp(0.2076) \\approx 1.23\\)\n\nThis implies that being a customer is associated with approximately a 23% increase in expected patent output, holding other variables constant.\n\n\n\nBecause the coefficients are on a log scale, we simulate two counterfactual scenarios:\n\n\\(X_0\\): All firms are treated as non-customers (iscustomer = 0)\n\\(X_1\\): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nWe use our fitted model to compute:\n\n\\(\\hat{y}_0\\) = predicted patents for \\(X_0\\)\n\\(\\hat{y}_1\\) = predicted patents for \\(X_1\\)\nAverage difference = \\(\\hat{y}_1 - \\hat{y}_0\\)\n\n\\[\n\\text{Average difference} = 0.79\n\\]\nThis means Blueprinty customers are predicted to earn 0.79 more patents, on average, than they would have if they weren’t customers.\n\n\n\nOur analysis suggests that Blueprinty’s software has a positive and meaningful effect on patent success. While the impact of being a customer translates to fewer than one additional patent over five years, this is a substantial gain in innovation output, especially when aggregated across a large number of firms. These results highlight Blueprinty’s potential to support R&D productivity."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('blueprinty.csv')\ndf.head()\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('blueprinty.csv')\nmeans = df.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmeans.columns = [\"Customer Status\", \"Mean Patents\"]\n\nplt.figure(figsize=(10, 5))\n\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=30,\n             element=\"step\", stat=\"density\", common_norm=False,\n             palette={0: \"skyblue\", 1: \"orange\"}, legend=True)\n\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status\nMean Patents\n\n\n\n\nNon-Customer\n3.473013\n\n\nCustomer\n4.133056\n\n\n\nFrom the chart and table, we can see that Blueprinty customers tend to have more patents than non-customers. On average, customers have 4.13 patents, while non-customers have 3.47. The histogram shows that customers are more likely to appear in the higher end of the distribution.\nHowever, it’s important to remember that customers are not randomly selected. That means we cannot say for sure that being a customer causes someone to have more patents. It’s possible that customers are already different before joining—perhaps they are more experienced, innovative, or come from regions or industries with higher patent activity.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\") \n\nmean_age = df.groupby(\"iscustomer\")[\"age\"].mean().reset_index()\nmean_age.columns = [\"Customer Status\", \"Mean Age\"]\n\nregion_counts = pd.crosstab(df[\"region\"], df[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\nregion_props = region_counts.div(region_counts.sum(axis=0), axis=1).round(3) \n\nplt.figure(figsize=(10, 4))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", bins=20, palette=[\"orange\", \"skyblue\"], element=\"step\")\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\n\n\n\n\nNon-Customer (0)\n26.10\n\n\nCustomer (1)\n26.90\n\n\n\nCustomers tend to be slightly older than non-customers. The age distribution plot shows that customers are more concentrated in the 20–35 age range, while non-customers are more evenly spread out. Although the difference is modest, it suggests that age may play a role in who becomes a customer, and should be considered when comparing outcomes like patent ownership.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\") \n\nregion_counts = pd.crosstab(df[\"region\"], df[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\nregion_props = region_counts.div(region_counts.sum(axis=0), axis=1).round(3) \n\nregion_props.plot(kind=\"bar\", figsize=(10, 5), color=[\"#fdbf6f\", \"#a6cee3\"])  \nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Proportion within Group\")\nplt.legend(title=\"Customer Status\")\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nNon-Customer\nCustomer\n\n\n\n\nMidwest\n0.184\n0.077\n\n\nNortheast\n0.268\n0.682\n\n\nNorthwest\n0.155\n0.060\n\n\nSouth\n0.153\n0.073\n\n\nSouthwest\n0.240\n0.108\n\n\n\nThere are also notable differences in regional distribution. Customers are overwhelmingly concentrated in the Northeast (68%), whereas non-customers are more evenly distributed across regions, especially in the Midwest and Southwest. This uneven geographic pattern indicates that customer status is not random and may be influenced by location-based factors. As with age, region should be taken into account when analyzing differences between customers and non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nSuppose we observe independent draws \\(Y_1, Y_2, \\dots, Y_n \\sim \\text{Poisson}(\\lambda)\\),\nwhere the probability mass function of the Poisson distribution is:\n\\(f(Y_i \\mid \\lambda) = \\dfrac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThen, the likelihood function for the full sample is:\n\\(\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} f(Y_i \\mid \\lambda)\n= \\prod_{i=1}^{n} \\dfrac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\dfrac{1}{Y_i!}\\)\nSince it’s easier to work with, we usually take the logarithm of the likelihood function.\nThis gives us the log-likelihood:\n\\(\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n= -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\\)\nTo better understand the relationship between λ (lambda) and the observed data, we now walk through a full maximum likelihood estimation process for the Poisson model.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values  \n\n# ---- Step 1: Define log-likelihood function ----\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n\n# ---- Step 2: Plot lambda vs log-likelihood ----\nlambdas = np.linspace(0.1, 10, 200)\nloglik_values = [poisson_loglikelihood(lam, Y) for lam in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, loglik_values, label=\"Log-Likelihood\")\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label='MLE (mean of Y)')\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.show()\n\n# ---- Step 3: Analytical MLE solution ----\nlambda_mle_analytical = np.mean(Y)\n\n# ---- Step 4: Numerical MLE using optimization ----\nneg_loglik = lambda lam: -poisson_loglikelihood(lam, Y)\nopt_result = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle_numerical = opt_result.x\n\n# Collect results\nresults = {\n    \"Analytical MLE (mean of Y)\": round(lambda_mle_analytical, 4),\n    \"Numerical MLE (optimize)\": round(lambda_mle_numerical, 4)\n}\n\n\n\n\n\n\n\n\n\nThe plot above shows how the Poisson log-likelihood changes as we vary \\(\\lambda\\), the expected number of patents per firm. The curve reaches its peak at \\(\\lambda = 3.6847\\), which is the value that makes the observed data most likely — this is our Maximum Likelihood Estimate (MLE).\nThe table below confirms that both the analytical solution (sample mean) and the numerical optimization produce exactly the same MLE value. This gives us confidence that our model is correctly specified and our estimation is reliable.\n\n\n\nMethod\nMLE Value\n\n\n\n\nAnalytical MLE (mean of Y)\n3.6847\n\n\nNumerical MLE (optimize)\n3.6847\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\n\n# ---- Step 0: Load and prepare the data ----\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age_sq\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct the design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\ny = df[\"patents\"].values\nX_matrix = X.values\n\n# ---- Step 1: Define log-likelihood function with clipping to prevent overflow ----\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    xb = np.dot(X, beta)\n    xb = np.clip(xb, -50, 50)  # prevent overflow in exp\n    lam = np.exp(xb)\n    return np.sum(Y * xb - lam - gammaln(Y + 1))\n\n# ---- Step 2: Define negative log-likelihood for minimization ----\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\n# ---- Step 3: Numerical optimization to find MLE ----\nbeta_init = np.zeros(X.shape[1])\nresult = minimize(neg_loglikelihood, beta_init, args=(y, X_matrix), method=\"BFGS\")\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nse_hat = np.sqrt(np.diag(hessian_inv))\n\n# ---- Step 4: Present coefficient table ----\nresults_table = pd.DataFrame({\n    \"Coefficient\": beta_hat.round(4),\n    \"Std. Error\": se_hat.round(4)\n}, index=X.columns)\n\nresults_table\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.5100\n0.0591\n\n\nage\n0.1487\n0.0080\n\n\nage_sq\n-0.0030\n0.0002\n\n\niscustomer\n0.2076\n0.0289\n\n\nNortheast\n0.0292\n0.0540\n\n\nNorthwest\n-0.0176\n0.0668\n\n\nSouth\n0.0566\n0.0660\n\n\nSouthwest\n0.0506\n0.0597\n\n\n\n\n\nFrom our estimated Poisson regression model:\n\nThe coefficient on age is 0.1487, indicating that older firms tend to receive more patents.\nThe negative coefficient on age_sq (−0.0030) suggests diminishing returns to age — patenting increases with age but at a decreasing rate.\nThe coefficient on iscustomer is 0.2076, meaning that being a Blueprinty customer increases the log of expected patent counts by this amount.\n\nTo make this more interpretable, we exponentiate the iscustomer coefficient:\n\n\\(\\exp(0.2076) \\approx 1.23\\)\n\nThis implies that being a customer is associated with approximately a 23% increase in expected patent output, holding other variables constant.\n\n\n\nBecause the coefficients are on a log scale, we simulate two counterfactual scenarios:\n\n\\(X_0\\): All firms are treated as non-customers (iscustomer = 0)\n\\(X_1\\): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nWe use our fitted model to compute:\n\n\\(\\hat{y}_0\\) = predicted patents for \\(X_0\\)\n\\(\\hat{y}_1\\) = predicted patents for \\(X_1\\)\nAverage difference = \\(\\hat{y}_1 - \\hat{y}_0\\)\n\n\\[\n\\text{Average difference} = 0.79\n\\]\nThis means Blueprinty customers are predicted to earn 0.79 more patents, on average, than they would have if they weren’t customers.\n\n\n\nOur analysis suggests that Blueprinty’s software has a positive and meaningful effect on patent success. While the impact of being a customer translates to fewer than one additional patent over five years, this is a substantial gain in innovation output, especially when aggregated across a large number of firms. These results highlight Blueprinty’s potential to support R&D productivity."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\ndf.head()\n\n\n\n\nData Exploration and Cleaning\nBefore modeling, we explore and clean the dataset to ensure that it contains only relevant and usable observations. We also perform some basic exploratory data analysis (EDA) to better understand the distribution and relationships among key variables.\nBefore we begin analysis or plotting, it’s important to understand which variables have missing values and how significant the gaps are.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\nmissing_pct = df.isnull().mean().sort_values(ascending=False) * 100\n\n# Plot missingness\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nmissing_pct[missing_pct &gt; 0].plot(kind='barh', color='skyblue')\nplt.title(\"Missing Data by Column\")\nplt.xlabel(\"% missing\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nApproximately 25% of listings are missing values in at least one of the review score fields — including cleanliness, location, or value ratings. In contrast, the rest of the dataset — including key variables such as price, bathrooms, bedrooms, and room_type — is largely complete, with fewer than 1% missing values. This gives us confidence that we can move forward with a clean dataset by removing only rows with missing review scores, while preserving most of the original sample.\nTo understand the distribution of listing types, we first look at how many units fall into each room category.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_clean = df.dropna(subset=columns_to_keep)\ndf_clean[\"room_type\"].value_counts()\n\n\n\n\n\nRoom Type\nCount\n\n\n\n\nEntire home/apt\n15,543\n\n\nPrivate room\n13,773\n\n\nShared room\n844\n\n\n\nWe see that most listings are either entire homes or private rooms. Shared rooms are relatively rare in the dataset, making up less than 3% of listings.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\ndf.isnull().sum()\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\n\ndf_clean = df.dropna(subset=columns_to_keep)\ndf_clean.shape\n\ndf_clean[\"price\"] = df_clean[\"price\"].replace(\"[\\$,]\", \"\", regex=True).astype(float)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean = pd.concat([df_clean, room_dummies], axis=1)\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean = pd.concat([df_clean, room_dummies], axis=1)\ndf_clean[[\"price\", \"bathrooms\", \"bedrooms\", \"number_of_reviews\"]].describe()\n\n\nNext, we explore the core numeric variables such as price, bedrooms, and number of reviews. This gives us a sense of the data’s central tendency, spread, and potential outliers.\n\n\n\n\n\n\n\n\n\n\nStatistic\nprice\nbathrooms\nbedrooms\nnumber_of_reviews\n\n\n\n\nCount\n30160\n30160\n30160\n30160\n\n\nMean\n140.21\n1.12\n1.15\n21.17\n\n\nStd. Dev.\n188.39\n0.38\n0.70\n32.01\n\n\nMin\n10.00\n0.00\n0.00\n1.00\n\n\n25%\n70.00\n1.00\n1.00\n3.00\n\n\n50%\n103.00\n1.00\n1.00\n8.00\n\n\n75%\n169.00\n1.00\n1.00\n26.00\n\n\nMax\n10000.00\n6.00\n10.00\n421.00\n\n\n\nThe average nightly price for a listing is $140, but prices vary widely, with some as low as $10 and a few as high as $10,000 — indicating potential outliers. Most listings have just one bathroom and bedroom. The average number of reviews is 21.17, but the maximum reaches 421, which suggests a strong right-skew in review counts. This supports the idea that some listings are booked (and reviewed) far more frequently than others.\n\nWe begin by exploring the overall distribution of review counts across listings. Since number_of_reviews is our proxy for bookings, it’s important to understand how this outcome behaves.\nThe histogram below shows a highly right-skewed distribution. Most listings receive fewer than 30 reviews, with a large number of listings receiving fewer than 10. A small number of listings stand out with over 200 reviews — these will have a large influence on our model.\n\n\n\n\n\n\nWhy This Matters\n\n\n\nThe skewed distribution confirms that a Poisson model — which is well-suited for modeling count data — is appropriate for this task.\n\n\nWe visualize the overall distribution of review counts to further confirm its skewness. This outcome variable will serve as our target in the Poisson regression.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_clean = df.dropna(subset=columns_to_keep)\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe now compare review activity across different room types to see if guests prefer specific listing categories. The boxplot shows how the number of reviews differs by room type. Private rooms and entire homes/apartments tend to have a wider spread of review counts, with many listings receiving over 100 reviews. Shared rooms receive fewer reviews on average. This suggests that travelers may prefer private or full-space options, and that those listings are likely booked more often.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\n\ndf_clean = df.dropna(subset=columns_to_keep)\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=df_clean)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\n\nTo explore whether price affects popularity, we examine the relationship between listing price and number of reviews. The scatterplot shows a weak inverse relationship between listing price and number of reviews. Overall, we see that lower-priced listings tend to have more reviews, while higher-priced listings receive fewer. This inverse relationship is intuitive — cheaper listings are more accessible to a wider range of guests, potentially leading to higher booking and review rates. However, there is still significant variability, especially among listings priced under $200.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_clean = df.dropna(subset=columns_to_keep)\n\nsns.scatterplot(x=\"price\", y=\"number_of_reviews\", data=df_clean)\nplt.xlim(0, 500)  # Remove extreme outliers\nplt.title(\"Price vs Number of Reviews\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFinally, we look at how numeric variables relate to one another. This helps identify strong pairwise relationships and avoid multicollinearity in modeling.\nKey observations:\n\nreview_scores_cleanliness and review_scores_value have a relatively strong positive correlation (~0.62).\nprice and number_of_reviews are almost uncorrelated (~−0.002), which supports what we observed visually.\nCleanliness and location scores tend to correlate moderately with other review-based features.\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_clean = df.dropna(subset=columns_to_keep)\nsns.heatmap(df_clean[[\n    \"price\", \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\"\n]].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom these exploratory insights, we observe several patterns worth testing formally. In the next section, we fit a Poisson regression model to quantify which features are most associated with review count.\n\n\nModeling: Poisson Regression\nWe now build a Poisson regression model to understand what factors are associated with higher booking activity on Airbnb, using number_of_reviews as a proxy for the number of bookings.\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Load and clean dataset\ndf = pd.read_csv(\"airbnb.csv\")\ncolumns_to_keep = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf_clean = df.dropna(subset=columns_to_keep)\ndf_clean[\"price\"] = df_clean[\"price\"].replace(\"[\\$,]\", \"\", regex=True).astype(float)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean = pd.concat([df_clean, room_dummies], axis=1)\n\n# Define X and y\nX_model = df_clean[[\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\", \"Private room\", \"Shared room\"\n]]\nX_model = sm.add_constant(X_model).astype(float)\ny_model = df_clean[\"number_of_reviews\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(y_model, X_model, family=sm.families.Poisson()).fit()\npoisson_model.summary()\n\n\nThe table below summarizes the estimated coefficients and their interpretation. Coefficients are in log scale, so we exponentiate them to interpret as % change in expected review count.\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\nInterpretation\n\n\n\n\nIntercept\n3.5554\n0.015\n0.000\nBaseline log review count\n\n\nPrice\n−0.0000022\n0.0000075\n0.769\nNo significant effect\n\n\nCleanliness Score\n0.1135\n0.001\n0.000\nHigher cleanliness → more reviews\n\n\nLocation Score\n−0.0794\n0.002\n0.000\nSlightly lower reviews with higher location\n\n\nValue Score\n−0.0921\n0.002\n0.000\nSlightly lower reviews with higher value\n\n\nInstant Bookable\n0.3331\n0.003\n0.000\n+40% more reviews if instantly bookable\n\n\nPrivate Room\n−0.0319\n0.003\n0.000\n~3% fewer reviews than Entire home/apt\n\n\nShared Room\n−0.2719\n0.009\n0.000\n~24% fewer reviews than Entire home/apt\n\n\n\n\nThe price variable does not have a statistically significant effect on the number of reviews.\nReview scores for cleanliness have a positive effect, while scores for location and value have slightly negative effects.\nListings that are instantly bookable tend to get around 40% more reviews, which suggests higher guest interest or ease of booking.\nCompared to entire homes/apartments (the reference room type), private rooms and especially shared rooms tend to receive fewer reviews, suggesting guests may prefer more private accommodations.\n\n\n\nResult and Conclusion\nAfter cleaning the dataset and conducting exploratory analysis, we used a Poisson regression model to identify which features are most associated with booking activity, as measured by the number of reviews. The model helps us understand how listing characteristics relate to the volume of guest interest and interaction.\nBased on the regression output, we observed several important insights:\n\nCleanliness score: Each one-point increase in cleanliness is associated with approximately 12% more reviews, highlighting the importance of guest perception of hygiene.\nInstant bookability: Listings that are instantly bookable tend to receive around 40% more reviews — likely due to convenience and ease of booking.\nRoom type:\n\nPrivate rooms receive 3% fewer reviews than entire homes/apartments (the baseline).\nShared rooms receive 24% fewer reviews, suggesting a clear guest preference for more privacy.\n\nPrice: Surprisingly, price does not have a statistically significant effect on review count in this dataset.\nLocation and value scores: Both showed statistically significant but slightly negative effects on reviews — possibly due to nonlinear relationships or correlation with other variables.\n\nThese results suggest that Airbnb hosts aiming to increase booking activity should focus on the features that matter most to guests:\n\nEnable instant booking,\nImprove cleanliness standards\nOffer more private spaces.\n\nAdjusting the listing price may be less effective on its own, especially if other core features do not meet guest expectations. These findings align with common user behavior — convenience, comfort, and perceived quality often drive engagement more than price alone."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/index.html#simulate-conjoint-data",
    "href": "blog/project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n#| eval: false\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/project3/index.html#preparing-the-data-for-estimation",
    "href": "blog/project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('conjoint_data.csv')\n\n# Step 1: Create dummy variables for brand and ad (drop reference level)\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Step 2: Define feature matrix X\n# Reference: brand_H, ad_No are omitted\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\n\n# Step 3: Add intercept manually\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]  # ensure order\n\n# Step 4: Create outcome vector y (binary: 1 if chosen, 0 otherwise)\ny = df_encoded[\"choice\"]\n\n# Step 5: Group metadata (optional but useful for tracking)\ngroups = df_encoded[[\"resp\", \"task\"]]\n\n# Preview\nX.head(), y.head(), groups.head()"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe estimate the model using Maximum Likelihood. First, we define the log-likelihood function based on the MNL specification, then use numerical optimization to find the MLEs. Finally, we use the inverse Hessian to obtain standard errors and construct 95% confidence intervals.\n\nLog-Likelihood Function\n\nimport numpy as np\nfrom scipy.special import logsumexp\nimport pandas as pd\ndf = pd.read_csv('conjoint_data.csv')\n\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]  # ensure order\ny = df_encoded[\"choice\"]\ngroups = df_encoded[[\"resp\", \"task\"]]\n\n# Reshape for 3-alternative tasks\nX_np = X.values\ny_np = y.values\nX_tasks = X_np.reshape((-1, 3, X_np.shape[1]))\ny_tasks = y_np.reshape((-1, 3))\n\ndef neg_log_likelihood(beta):\n    \"\"\"Negative log-likelihood for MNL.\"\"\"\n    X_tasks = X_tasks.astype(np.float64)\n    beta = np.asarray(beta, dtype=np.float64)\n    utilities = np.einsum(\"tjk,k-&gt;tj\", X_tasks, beta)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)\n    return -np.sum(chosen_log_probs)\n\n\n\nOptimization and Estimation Results\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\n\ndf = pd.read_csv(\"conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].copy()\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\ny = df_encoded[\"choice\"]\nX_np = X.values.astype(np.float64)\ny_np = y.values.astype(np.float64)\nX_tasks = X_np.reshape((-1, 3, X_np.shape[1]))\ny_tasks = y_np.reshape((-1, 3))\n\ndef neg_log_likelihood(beta):\n    utilities = np.einsum(\"tjk,k-&gt;tj\", X_tasks, beta)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)\n    return -np.sum(chosen_log_probs)\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=np.zeros(X_np.shape[1]),\n    method=\"BFGS\"\n)\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_err = np.sqrt(np.diag(hessian_inv))\nz = 1.96\nci_lower = beta_hat - z * std_err\nci_upper = beta_hat + z * std_err\n\nparam_names = [\"Intercept\", \"Netflix\", \"Prime\", \"Ad\", \"Price\"]\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std. Error\": std_err,\n    \"95% CI Lower\": ci_lower,\n    \"95% CI Upper\": ci_upper\n}, index=param_names).round(4)\n\nThe table below summarizes the estimated coefficients from the MNL model, along with standard errors and 95% confidence intervals.\n\n\n\nVariable\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.0000\n1.0000\n-1.9600\n1.9600\n\n\nNetflix\n0.9412\n0.1181\n0.7097\n1.1727\n\n\nPrime\n0.5016\n0.1207\n0.2651\n0.7382\n\n\nAd\n-0.7320\n0.0893\n-0.9071\n-0.5569\n\n\nPrice\n-0.0995\n0.0063\n-0.1119\n-0.0871\n\n\n\nThe results indicate strong preferences for Netflix and Prime (relative to Hulu), a penalty for ads, and a negative effect of price. All coefficients are statistically significant except for the intercept."
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-bayesian-methods",
    "href": "blog/project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nPosterior Simulation: Metropolis-Hastings\nWe use a Metropolis-Hastings MCMC sampler to simulate draws from the posterior distribution of our model parameters. We take 11,000 steps, discard the first 1,000 as burn-in, and retain 10,000 posterior draws for inference.\nThe proposal distribution is a multivariate normal with independent dimensions. The first three parameters (Netflix, Prime, Ad) use a standard deviation of 0.05, and the price coefficient uses 0.005, reflecting tighter prior belief.\nOur sampler achieved an acceptance rate of 56.40%, which is in the desirable range for good mixing.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\nfrom scipy.optimize import minimize\n\n# Load and encode data\ndf = pd.read_csv(\"conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Create feature matrix X\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].copy()\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\n\n# Create target vector y\ny = df_encoded[\"choice\"]\n\n# Convert to NumPy arrays\nX_np = X.values.astype(np.float64)\ny_np = y.values.astype(np.float64)\nX_tasks = X_np.reshape((-1, 3, X_np.shape[1]))\ny_tasks = y_np.reshape((-1, 3))\n\n# Log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = np.einsum(\"tjk,k-&gt;tj\", X_tasks, beta)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)\n    return -np.sum(chosen_log_probs)\n\n# Log-prior (N(0,5) for binary, N(0,1) for price)\ndef log_prior(beta):\n    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)\n    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)\n    return np.sum(binary_prior) + price_prior\n\n# Log-posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings MCMC sampler\ndef metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):\n    n_params = len(initial_beta)\n    samples = np.zeros((steps, n_params))\n    accepted = 0\n    current_beta = initial_beta\n    current_log_post = log_posterior(current_beta)\n\n    if proposal_scales is None:\n        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\n\n    for step in range(steps):\n        proposal = current_beta + np.random.normal(scale=proposal_scales)\n        proposal_log_post = log_posterior(proposal)\n        accept_ratio = np.exp(proposal_log_post - current_log_post)\n        if np.random.rand() &lt; accept_ratio:\n            current_beta = proposal\n            current_log_post = proposal_log_post\n            accepted += 1\n        samples[step] = current_beta\n\n    print(f\"Acceptance rate: {accepted / steps:.2%}\")\n    return samples\n\n# Run MCMC\ninitial_beta = np.zeros(X_np.shape[1])\nproposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\nsamples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)\n\n# Discard burn-in\nposterior_samples = samples[1000:]\nposterior_samples\n\n\nThe result is a NumPy array of shape (10,000, 5), where each column represents a parameter:\n\n\n\nIndex\nParameter\n\n\n\n\n0\nIntercept\n\n\n1\nNetflix\n\n\n2\nPrime\n\n\n3\nAd\n\n\n4\nPrice\n\n\n\n\n\nPosterior Visualization: Price Coefficient\nWe used a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the Price coefficient. The plots below include:\n\nA trace plot to assess convergence behavior\nA posterior histogram to visualize the shape and spread of the sampled values\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\nfrom scipy.optimize import minimize\n\n# Load and encode data\ndf = pd.read_csv(\"conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Create feature matrix X\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].copy()\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\n\n# Create target vector y\ny = df_encoded[\"choice\"]\n\n# Convert to NumPy arrays\nX_np = X.values.astype(np.float64)\ny_np = y.values.astype(np.float64)\nX_tasks = X_np.reshape((-1, 3, X_np.shape[1]))\ny_tasks = y_np.reshape((-1, 3))\n\n# Log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = np.einsum(\"tjk,k-&gt;tj\", X_tasks, beta)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)\n    return -np.sum(chosen_log_probs)\n\n# Log-prior (N(0,5) for binary, N(0,1) for price)\ndef log_prior(beta):\n    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)\n    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)\n    return np.sum(binary_prior) + price_prior\n\n# Log-posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings MCMC sampler\ndef metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):\n    n_params = len(initial_beta)\n    samples = np.zeros((steps, n_params))\n    accepted = 0\n    current_beta = initial_beta\n    current_log_post = log_posterior(current_beta)\n\n    if proposal_scales is None:\n        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\n\n    for step in range(steps):\n        proposal = current_beta + np.random.normal(scale=proposal_scales)\n        proposal_log_post = log_posterior(proposal)\n        accept_ratio = np.exp(proposal_log_post - current_log_post)\n        if np.random.rand() &lt; accept_ratio:\n            current_beta = proposal\n            current_log_post = proposal_log_post\n            accepted += 1\n        samples[step] = current_beta\n\n    print(f\"Acceptance rate: {accepted / steps:.2%}\")\n    return samples\n\n# Run MCMC\ninitial_beta = np.zeros(X_np.shape[1])\nproposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\nsamples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)\n\n# Discard burn-in\nposterior_samples = samples[1000:]\nposterior_samples\n\n# Set parameter names\nparam_names = [\"Intercept\", \"Netflix\", \"Prime\", \"Ad\", \"Price\"]\n\n# Choose one parameter to plot (e.g., Price = index 4)\nparam_index = 4\nparam_label = param_names[param_index]\nsamples_param = posterior_samples[:, param_index]\n\n# Create figure with trace and histogram\nfig, axes = plt.subplots(2, 1, figsize=(10, 6), constrained_layout=True)\n\n# Trace plot\naxes[0].plot(samples_param, alpha=0.8, linewidth=0.8)\naxes[0].set_title(f\"Trace Plot for {param_label} Coefficient\")\naxes[0].set_ylabel(\"Value\")\naxes[0].set_xlabel(\"Iteration\")\n\n# Histogram\nsns.histplot(samples_param, bins=50, kde=True, ax=axes[1], color=\"skyblue\")\naxes[1].set_title(f\"Posterior Distribution of {param_label} Coefficient\")\naxes[1].set_xlabel(\"Coefficient Value\")\naxes[1].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\nAcceptance rate: 57.38%\n\n\n/var/folders/yf/g_800hts37z9ftxfvzrc1fc80000gn/T/ipykernel_2475/2474500630.py:99: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nThe trace plot shows stable, well-mixed values with no drift — indicating convergence and good mixing.\nThe posterior distribution is approximately normal and centered near −0.10.\nThis supports our interpretation that increasing price reduces utility, aligning well with economic theory and the MLE result.\n\n\n\n\nFeature\nObservation\n\n\n\n\nTrace behavior\nWell-mixed, stationary — good convergence\n\n\nPosterior shape\nBell-shaped, symmetric\n\n\nPosterior center\nAround −0.10\n\n\nInterpretation\nPrice reduces utility in choice behavior\n\n\n\n\n\nPosterior Summary: Bayesian Estimation via MCMC\nAfter running 11,000 Metropolis-Hastings iterations and discarding the first 1,000 as burn-in, we retained 10,000 samples from the posterior distribution for each parameter.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\nfrom scipy.optimize import minimize\n\n# Load and encode data\ndf = pd.read_csv(\"conjoint_data.csv\")\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Create feature matrix X\nX = df_encoded[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].copy()\nX[\"intercept\"] = 1\nX = X[[\"intercept\", \"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]]\n\n# Create target vector y\ny = df_encoded[\"choice\"]\n\n# Convert to NumPy arrays\nX_np = X.values.astype(np.float64)\ny_np = y.values.astype(np.float64)\nX_tasks = X_np.reshape((-1, 3, X_np.shape[1]))\ny_tasks = y_np.reshape((-1, 3))\n\n# Log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = np.einsum(\"tjk,k-&gt;tj\", X_tasks, beta)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    chosen_log_probs = np.sum(log_probs * y_tasks, axis=1)\n    return -np.sum(chosen_log_probs)\n\n# Log-prior (N(0,5) for binary, N(0,1) for price)\ndef log_prior(beta):\n    binary_prior = -0.5 * (beta[1:4] ** 2) / (5 ** 2)\n    price_prior = -0.5 * (beta[4] ** 2) / (1 ** 2)\n    return np.sum(binary_prior) + price_prior\n\n# Log-posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings MCMC sampler\ndef metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=None):\n    n_params = len(initial_beta)\n    samples = np.zeros((steps, n_params))\n    accepted = 0\n    current_beta = initial_beta\n    current_log_post = log_posterior(current_beta)\n\n    if proposal_scales is None:\n        proposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\n\n    for step in range(steps):\n        proposal = current_beta + np.random.normal(scale=proposal_scales)\n        proposal_log_post = log_posterior(proposal)\n        accept_ratio = np.exp(proposal_log_post - current_log_post)\n        if np.random.rand() &lt; accept_ratio:\n            current_beta = proposal\n            current_log_post = proposal_log_post\n            accepted += 1\n        samples[step] = current_beta\n\n    print(f\"Acceptance rate: {accepted / steps:.2%}\")\n    return samples\n\n# Run MCMC\ninitial_beta = np.zeros(X_np.shape[1])\nproposal_scales = np.array([0.05, 0.05, 0.05, 0.05, 0.005])\nsamples = metropolis_hastings(log_posterior, initial_beta, steps=11000, proposal_scales=proposal_scales)\n\n# Discard burn-in\nposterior_samples = samples[1000:]\n# Compute posterior statistics\nposterior_mean = np.mean(posterior_samples, axis=0)\nposterior_std = np.std(posterior_samples, axis=0)\nposterior_ci_lower = np.percentile(posterior_samples, 2.5, axis=0)\nposterior_ci_upper = np.percentile(posterior_samples, 97.5, axis=0)\n\n# Organize into a DataFrame\nparam_names = [\"Intercept\", \"Netflix\", \"Prime\", \"Ad\", \"Price\"]\nposterior_summary = pd.DataFrame({\n    \"Posterior Mean\": posterior_mean,\n    \"Posterior Std. Dev\": posterior_std,\n    \"95% Credible Interval Lower\": posterior_ci_lower,\n    \"95% Credible Interval Upper\": posterior_ci_upper\n}, index=param_names).round(4)\n\nposterior_summary\n\n\nThe table below reports the posterior mean, standard deviation, and 95% credible interval for each of the 5 model parameters.\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.7741\n0.8863\n−0.7093\n2.5075\n\n\nNetflix\n0.9471\n0.1139\n0.7332\n1.1767\n\n\nPrime\n0.5058\n0.1162\n0.2877\n0.7364\n\n\nAd\n−0.7380\n0.0860\n−0.9088\n−0.5750\n\n\n\n\nAll posterior means are very close to your MLE estimates.\nCredible intervals are tight and consistent with maximum likelihood confidence intervals.\nThe Netflix and Prime coefficients remain positive and significant, while Ad and Price are negative as expected.\nThis reinforces that your MCMC sampler worked well, and confirms your results from the MLE approach."
  },
  {
    "objectID": "blog/project3/index.html#discussion",
    "href": "blog/project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nParameter Interpretation (as if data were not simulated)\nLet’s suppose we were working with real-world data instead of simulated conjoint responses. Based on the parameter estimates, we can still derive meaningful insights:\n\n\\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\): This implies that, all else equal, consumers prefer Netflix over Amazon Prime. The higher coefficient for Netflix indicates that it contributes more to overall utility than Prime when making a choice.\n\\(\\beta_\\text{Ad} &lt; 0\\): This negative value suggests that respondents dislike advertisements — products with ads are less likely to be chosen than ad-free options.\n\\(\\beta_\\text{Price} &lt; 0\\): This makes intuitive sense: higher price reduces the likelihood of choice, all else being equal. A negative price coefficient is expected in virtually any consumer utility model.\n\nIf we hadn’t known this data was simulated, we would still reasonably conclude the following:\n\nConsumers exhibit clear brand preferences, with Netflix most preferred.\nAd-free plans are consistently more attractive.\nPrice is a deterrent, confirming basic economic intuition.\n\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  }
]